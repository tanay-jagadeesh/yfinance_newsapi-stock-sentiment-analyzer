import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, root_mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import TimeSeriesSplit
from xgboost import XGBRegressor

# Load train/val/test sets
X_train = pd.read_csv('X_train.csv')
X_val = pd.read_csv('X_val.csv')
X_test = pd.read_csv('X_test.csv')

y_train = pd.read_csv('y_train.csv')
y_val = pd.read_csv('y_val.csv')
y_test = pd.read_csv('y_test.csv')

# Drop text columns (can't train on text)
if 'titles_combined' in X_train.columns:
    X_train = X_train.drop('titles_combined', axis=1)
    X_val = X_val.drop('titles_combined', axis=1)
    X_test = X_test.drop('titles_combined', axis=1)

#created model
reg_model = LinearRegression()

#trained model
reg_model.fit(X_train, y_train)

#predictions
reg_model_predictions = reg_model.predict(X_val)

# Decision Tree Model
dt_model = DecisionTreeRegressor()
dt_model.fit(X_train, y_train)
dt_predictions = dt_model.predict(X_val)

# Random Forest Model
rf_model = RandomForestRegressor()
rf_model.fit(X_train, y_train)
rf_predictions = rf_model.predict(X_val)

#Mean Squared Error for Lin Reg
lr_mse = mean_squared_error(y_val, reg_model_predictions)
print(f"Linear Regression MSE: {lr_mse}")

#Mean Squared Error for decision tree
dt_mse = mean_squared_error(y_val, dt_predictions)
print(f"Decision Tree MSE: {dt_mse}")

#Mean Squared Error for random forest
rf_mse = mean_squared_error(y_val, rf_predictions)
print(f"Random Forest MSE: {rf_mse}")

#Root Mean Squared Error for Lin Reg
lr_rmse = root_mean_squared_error(y_val, reg_model_predictions)
print(f"Linear Regression RMSE: {lr_rmse}")

#Root Mean Squared Error for decision tree
dt_rmse = root_mean_squared_error(y_val, dt_predictions)
print(f"Decision Tree RMSE: {dt_rmse}")

#Root Mean Squared Error for random forest
rf_rmse = root_mean_squared_error(y_val, rf_predictions)
print(f"Random Forest RMSE: {rf_rmse}")

#R² Score for Lin Reg
lr_r2 = r2_score(y_val, reg_model_predictions)
print(f"Linear Regression R²: {lr_r2}")

#R² Score for decision tree
dt_r2 = r2_score(y_val, dt_predictions)
print(f"Decision Tree R²: {dt_r2}")

#R² Score for random forest
rf_r2 = r2_score(y_val, rf_predictions)
print(f"Random Forest R²: {rf_r2}")

#MAE for Lin Reg
lr_mae = mean_absolute_error(y_val, reg_model_predictions)
print(f"Linear Regression MAE: {lr_mae}")

#MAE for decision tree
dt_mae = mean_absolute_error(y_val, dt_predictions)
print(f"Decision Tree MAE: {dt_mae}")

#MAE for random forest
rf_mae = mean_absolute_error(y_val, rf_predictions)
print(f"Random Forest MAE: {rf_mae}")

# comparing models and seeing which performed well
comparison_df = pd.DataFrame({
    'Model': ['Linear Regression', 'Decision Tree', 'Random Forest'],
    'MSE': [lr_mse, dt_mse, rf_mse],
    'RMSE': [lr_rmse, dt_rmse, rf_rmse],
    'R²': [lr_r2, dt_r2, rf_r2],
    'MAE': [lr_mae, dt_mae, rf_mae]
})

# searching best model
print("\nBest Models by Metric:")
print(f"  Lowest MSE: {comparison_df.loc[comparison_df['MSE'].idxmin(), 'Model']}")
print(f"  Lowest RMSE: {comparison_df.loc[comparison_df['RMSE'].idxmin(), 'Model']}")
print(f"  Highest R²: {comparison_df.loc[comparison_df['R²'].idxmax(), 'Model']}")
print(f"  Lowest MAE: {comparison_df.loc[comparison_df['MAE'].idxmin(), 'Model']}")

# saved results to csv file
comparison_df.to_csv('model_comparison.csv', index=False)

#Gradient Boosting model

gbc = GradientBoostingRegressor()
gbc.fit(X_train, y_train)
gbc_predictions = gbc.predict(X_val)

#XGboost model

xgb_model = XGBRegressor()
xgb_model.fit(X_train, y_train)
xgb_predictions = xgb_model.predict(X_val)

#Time Series Split (cross val.)

tscv = TimeSeriesSplit(n_splits = 5)

#Random Forest Regressor
for train_index, val_index in (tscv.split(X_train)):
    X_train_fold = X_train.iloc[train_index]
    y_train_fold = y_train.iloc[train_index]
    X_val_fold = X_train.iloc[val_index]
    y_val_fold = y_train.iloc[val_index]

    model = RandomForestRegressor()

    model.fit(X_train_fold, y_train_fold)

    model_predictions = model.predict(X_val_fold)
    score = r2_score(y_val_fold, model_predictions)
    print(f"RF Fold score: {score}")

#Decision Tree Regressor
for train_index, val_index in (tscv.split(X_train)):
    X_train_fold = X_train.iloc[train_index]
    y_train_fold = y_train.iloc[train_index]
    X_val_fold = X_train.iloc[val_index]
    y_val_fold = y_train.iloc[val_index]

    model = DecisionTreeRegressor()

    model.fit(X_train_fold, y_train_fold)

    model_predictions = model.predict(X_val_fold)
    score = r2_score(y_val_fold, model_predictions)
    print(f"DT Fold score: {score}")

#Linear Regressor
for train_index, val_index in (tscv.split(X_train)):
    X_train_fold = X_train.iloc[train_index]
    y_train_fold = y_train.iloc[train_index]
    X_val_fold = X_train.iloc[val_index]
    y_val_fold = y_train.iloc[val_index]

    model = LinearRegression()

    model.fit(X_train_fold, y_train_fold)

    model_predictions = model.predict(X_val_fold)
    score = r2_score(y_val_fold, model_predictions)
    print(f"LR Fold score: {score}")

#Gradient Boosting Regressor
for train_index, val_index in (tscv.split(X_train)):
    X_train_fold = X_train.iloc[train_index]
    y_train_fold = y_train.iloc[train_index]
    X_val_fold = X_train.iloc[val_index]
    y_val_fold = y_train.iloc[val_index]

    model = GradientBoostingRegressor()

    model.fit(X_train_fold, y_train_fold)

    model_predictions = model.predict(X_val_fold)
    score = r2_score(y_val_fold, model_predictions)
    print(f"GB Fold score: {score}")

#xgboost
for train_index, val_index in (tscv.split(X_train)):
    X_train_fold = X_train.iloc[train_index]
    y_train_fold = y_train.iloc[train_index]
    X_val_fold = X_train.iloc[val_index]
    y_val_fold = y_train.iloc[val_index]

    model = XGBRegressor()

    model.fit(X_train_fold, y_train_fold)

    model_predictions = model.predict(X_val_fold)
    score = r2_score(y_val_fold, model_predictions)
    print(f"XGB Fold score: {score}")

# want to see which performs the best 
best_model = comparison_df.loc[comparison_df['R²'].idxmax(), 'Model']
print(f"\nOverall Best Model: {best_model}")